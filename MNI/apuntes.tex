\part{Tema 1. Introducción a los problemas del Análisis Numérico}

\section{Introducción a los métodos numéricos: algoritmo}
Comenzaremos con un ejemplo de recurrencia en el que observaremos que al redondear el primer valor, se acumula el error y los siguientes valores se desbordan.\\
Sea n $\geq$ 1 y la recurrencia $x_{n}$:=$\int_{0}^{1} x^{n}e^{x}dx$.\\
Si resolvemos la integral para n = 0, tenemos que $x_{0}$ = e - 1\\
Y para n $\in$ $\mathbb{N}$, tenemos que $x_{n}$ = e - n$x_{n-1}$\\
Por lo que esta sucesión, $\lbrace x_{n} \rbrace_{n\geq1}$ $\subset$ $\mathbb{R_{+}}$, es decreciente y tiende a 0, es decir, $ \lim_{n \to \infty} x_{n} = 0 $ \\
Veamos que si redondeamos $x_{0}$ se acumula el error.\\
Si n = 12 tenemos que $x_{12}$ = 0.1951\\
Redondeando $x_{0}$ = 1.7183 e iterando según este valor hasta n = 12, obtenemos que $x_{12}$ = 8704.39\\
Luego, este valor es, con diferencia, mayor que el que habíamos calculado sin redondeo y $x_{n}$ no tiende a 0.\\
Concluimos que el redondeo, a veces, conlleva errores muy grandes.

\subsection{Espacios normados}
Si usamos las normas en los problemas numéricos, sabremos si los problemas están bien planteados, los errores cometidos, la convergencia...\\

\begin{ndef}[Norma]
Sea E un espacio vectorial real, diremos que una aplicacion $\Vert$·$\Vert$: E $\rightarrow$ $\mathbb{R}$ es una $\textbf{norma}$ en E si verifica las siguientes propiedades:
	\begin{nlist}
	\item Sea x $\in$ E $\Rightarrow$ $\Vert$·$\Vert$ $\geq$ 0.\\
	Además, $\Vert$x$\Vert$ = 0 $\Leftrightarrow$ x = 0
	\item Sean x,y $\in$ E $\Rightarrow$ $\Vert$x + y$\Vert$ $\leq$ $\Vert$x$\Vert$ + $\Vert$y$\Vert$ $\quad$ (desigualdad triangular)
	\item Sean x $\in$ E, $\lambda$ $\in$ $\mathbb{R}$ $\Rightarrow$ $\Vert$ $	\lambda$x $\Vert$ = $\vert$ $\lambda$ $\vert$ $\Vert$x$\Vert$
	\end{nlist}
\end{ndef}

\begin{ndef}[Espacio normado]
Sea E un espacio vectorial real. Si este espacio admite una norma, entonces E se llama $\textbf{espacio normado}$.
\end{ndef}

Aunque trabajaremos en $\mathbb{R}$, en $\mathbb{C}$ es lo mismo.\\

Veamos la interpretación geométrica de la desigualdad triangular, usando la norma euclídea $\mathbb{R}^2$ o $\mathbb{R}^3$.\\
\includegraphics[scale=0.2]{media/desigualdadtriangular.png}

Las siguientes normas son las que vamos a utilizar.

\begin{ndef}[Norma p]
Sean E = $\mathbb{R}^N$, p $\geq$ 1 y x $\in$ $\mathbb{R}^N$, entonces:
\[ \Vert x \Vert _{p} := \left( \sum_{j=1}^{N} \vert x_{j} \vert ^p \right) ^{1/p} \]
Si p = 2, entonces la norma es $\textbf{euclídea}$.
\end{ndef}

\begin{ndef}[Norma del máximo]
Sea E = $\mathbb{R}^N$ y x $\in$ $\mathbb{R}^N$, entonces:
\[ \Vert x \Vert _{\infty} := max \lbrace \vert x_{j} \vert : j = 1,...,N \rbrace \]
\end{ndef}

\begin{ndef}[Norma de Frobenius]
Sea E = $\mathbb{R}^{M \times N}$ y A $\in$ $\mathbb{R}^{M \times N}$, entonces:
\[ \Vert A \Vert _F := \sqrt{\sum_{i=1}^{M} \sum_{j=1}^{N} a_{ij}^2} \]
\end{ndef}

$\textit{Aclaración:}$\\
$\textit{Si estamos en un espacio vectorial real C([a,b]), esto significa que este espacio está compuesto por todas las funciones}\\
\textit{continuas en el intervalo cerrado [a,b].}$\\
$\textit{Si el espacio es $C^k$([a,b]), significa que está formado por las funciones de clase k, es decir, funciones derivables hasta}\\
\textit{orden k y esas derivadas son continuas.}$

\begin{ndef}[Norma del máximo]
Sea E = C([a,b]) y f $\in$ C([a,b]), entonces:
\[ \Vert f \Vert _\infty := max \; \left\lbrace \vert f(x) \vert : a \leq x \leq b \right\rbrace \]
\end{ndef}

\begin{ndef}
Sean E = $C^k$([a,b]), k $\in$ $\mathbb{N}$ y f $\in$ $C^k$([a,b]) entonces:
\[ \Vert f \Vert _k := max \; \left\lbrace \Vert f^{(j)} \Vert : j = 0,...,k \right\rbrace \]
\end{ndef}

Ahora que ya tenemos definidas las normas, podemos calcular el error cometido al aproximar los vectores.

\begin{ndef}[Error absoluto]
Sea E un espacio normado, x $\in$ E y x* $\in$ una aproximación de x, entonces la siguiente operación calcula el error absoluto:
\[ \Vert x^* - x \Vert \]
\end{ndef}

\begin{ndef}[Error relativo]
Sea E un espacio normado, x $\in$ E y x* $\in$ una aproximación de x, entonces la siguiente operación calcula el error relativo:
\[  \frac {\Vert x^* - x \Vert}{ \Vert x \Vert} \]
\end{ndef}

Veamos una aplicación de estos errores.

\begin{ejer}
Calcula los errores absolutos y relativos de:
	\begin{nlist}
	\item E = $\mathbb{R}$, x = 1/4, x* = 0.23
	\item E = $\mathbb{R}^3$, x = (1/5,2,1), x* = (0.19,2.2,0.9)
	\item E = C([0,$\pi$/2]), f(t) = sen(t), f*(t) = t
	\end{nlist}
\end{ejer}

\begin{sol} $\newline$
	\begin{nlist}
	\item error absoluto: $\vert x^* - x \vert = \vert 0.23 - 1/4 \vert = 0.02 $\\
	error relativo: $\frac{\vert x^* - x \vert}{\vert x \vert} = \frac{\vert 0.23 - 1/4 \vert}{\vert 1/4 \vert} = 0.08$
	\item error absoluto: $\Vert x^* - x \Vert _\infty = \Vert (0.19,2.2,0.9) - (1/5,2,1) \Vert _\infty = \Vert (-0.01,0.2,-0.1) \Vert _\infty =$\\ $= max \lbrace 0.01,0.2,0.1\rbrace = 0.2 $\\
	error relativo: $\frac{\Vert x^* - x \Vert _\infty}{\Vert x \Vert _\infty} = \frac{\Vert (-0.01,0.2,-0.1) \Vert _\infty}{\Vert (1/5,2,1) \Vert _\infty} = \frac{0.2}{2} = 0.1 $\\
	\item error absoluto: $\Vert f^* - f \Vert _\infty = \Vert t - sen(t) \Vert _\infty = \frac{\pi}{2} - 1$\\
	error relativo: $\frac{\Vert f^* - f \Vert _\infty}{\Vert f \Vert _\infty} = \frac{\pi}{2} - 1 $
	\end{nlist}
\end{sol}

\begin{ndef}[Distancia]
Se define la $\textbf{distancia}$ entre dos vectores x,y $\in$ E como
\[ dist(x,y) := \Vert x - y \Vert \]
\end{ndef}

\begin{ndef}
Se dice que $\lbrace x_n \rbrace _{n \geq 1}$ en E $\textbf{converge}$ a $x_0$ $\in$ E sii
\[ \forall \varepsilon > 0 \Rightarrow \left[ \exists n_0 \in \mathbb{N} : n \geq n_0 \Rightarrow \Vert x_n - x_0 \Vert < \varepsilon \right] \]
es decir,
\[ \lim_{n \rightarrow \infty} x_n = x_0  \Leftrightarrow \lim_{n \rightarrow \infty} \Vert x_n - x_0 \Vert = 0 \]
\end{ndef}

\begin{ndef}
Sean X, Y subconjuntos no vacíos de sendos espacios normados y sea f: X $\rightarrow$ Y, diremos que f es $\textbf{continua}$ en $x_{0}$ $\in$ X si
\[ \forall \varepsilon > 0 \Rightarrow \left[ \exists \delta > 0 : x \in X \wedge \Vert x - x_0 \Vert < \delta \Rightarrow \Vert f(x) - f(x_0) \Vert < \varepsilon \right] \]
\end{ndef}

\begin{nprop}
Sea x $\in \mathbb{R}^N \Rightarrow \Vert x \Vert _\infty \leq \Vert x \Vert _1 \leq N\Vert x \Vert _\infty $ 
\end{nprop}

\begin{ndef}
Sean $\Vert$ · $\Vert$ y $\Vert$ · $\Vert _*$ dos normas, se dice que son $\textbf{equivalentes}$ si $\exists c_1, c_2 > 0$ tales que
\[ \forall x \in E \Rightarrow c_1\Vert x \Vert \leq \Vert x \Vert _* \leq c_2\Vert x \Vert \] 
\end{ndef}

\begin{nprop}
Sean $\Vert$ · $\Vert$ y $\Vert$ · $\Vert _*$ dos normas, entonces la convergencia de sucesiones y la continuidad son equivalentes para ambas normas. 
\end{nprop}

\begin{nth}
Todas las normas en un espacio normado finito dimensional son equivalentes.
\end{nth}

Observemos que para calcular el límite de la norma del máximo, tenemos que calcular el límite de cada coordenada.

\begin{nprop}
Sea $\mathbb{R}^N$ un espacio normado finito dimensional y consideremos la norma $\Vert$·$\Vert _\infty$ en este espacio, entonces:
\[ \lim_{n \rightarrow \infty}x_n = x_0 \Leftrightarrow \lim_{n \geq 1}(x_n)_j = (x_0)_j \qquad \forall j\in \lbrace 1,...,N \rbrace \]
\end{nprop}

\begin{proof}
\[ \lim_{n \rightarrow \infty} x_n = x_0 \quad \Leftrightarrow \quad \lim_{n \rightarrow \infty} \Vert x_n - x_0 \Vert _\infty = 0 \quad \Leftrightarrow \quad 0 < max \; \lbrace \vert (x_n - x_0)_j \vert : j = 1,...,N \rbrace = 0 \quad \rightarrow \quad \lim_{n \geq 1}(x_n)_j = (x_0)_j \]
\end{proof}

Un ejemplo de aplicación de esta proposición es el siguiente.

\begin{ejemplo}
$\lim_{n \geq 1} \left( \left( 1 + \frac{1}{n} \right) ^n , \frac{(-1)^n}{n^2} \right) = (e,0)$
\end{ejemplo}

La anterior proposición también se puede aplicar para cualquier norma de $\mathbb{R}^N$ y de $\mathbb{R}^{M \times N}$.

\begin{ejer}
Comprueba que la norma del máximo en C([0,1]) no es equivalente a la norma $\Vert$·$\Vert _1$ definida para cada f $\in$ C([0,1]) como
\[ \Vert f \Vert _1 := \int_0^1 \vert f(x) \vert dx \]
(Indicación: para cada n $\geq$ 2, considera la función $f_n$ cuya gráfica es la poligonal que une los puntos (0,0), (1/n,1), (2/n,0), (1,0)).
\end{ejer}

\begin{sol}
Tenemos que\\
\[ E = C([0,1]) \]
\[ \Vert f \Vert _\infty = max \left\lbrace \vert f(x) \vert : 0 \leq x \leq 1 \right\rbrace \]
\[ \Vert f \Vert _1 := \int_0^1 \vert f(x) \vert dx \]
Luego\\
$\Vert f_n \Vert _\infty = 1$ y $\Vert f_n \Vert _1 = 1/n$ (que coincide con el área).\\
Si fueran equivalentes, entonces
\[ \exists \alpha , \beta > 0 f \in E \Rightarrow \alpha \Vert f_n \Vert _1 \leq \Vert f_n \Vert _\infty \leq \beta \Vert f_n \Vert _1 \]
Lo cual es una contradicción, porque n $\geq$ 1 $\Rightarrow \Vert f_n \Vert _\infty \leq \beta \Vert f_n \Vert _1 \Leftrightarrow 1 \leq \frac{\beta}{n} \Leftrightarrow n \leq \beta$ $\;$ y n no está acotada.\\
Por lo que no son equivalentes.
\end{sol}

\begin{nprop}
Sean M, N $\in$ $\mathbb{N}$ y consideremos sendas normas en $\mathbb{R}^N$ y $\mathbb{R}^M$, que sin lugar a ambigüedad notaremos indeferentemente como $\Vert$·$\Vert$. Entonces la aplicación que notaremos igualmente como $\Vert$·$\Vert$ define una norma en $\mathbb{R}^{M \times N}$:
\[ \Vert A \Vert := sup \; \left\lbrace \Vert Ax \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{nprop}

\begin{ndef}
Se define la norma \textbf{inducida} en $\mathbb{R}^{M \times N}$ como:
\[ \Vert A \Vert := sup \; \left\lbrace \Vert Ax \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{ndef}

\begin{nprop}
Con la notación de la proposición anterior, si $A \in \mathbb{R}^{M \times N}$ entonces
\[ \Vert A \Vert := sup \; \left\lbrace \frac{\Vert Ax \Vert}{\Vert x \Vert } : x \in \mathbb{R}^N \wedge x \neq 0 \right\rbrace \]
En particular,
\[ \Vert Ax \Vert \leq \Vert A \Vert \Vert x \Vert \]
\end{nprop}

\begin{nprop}
Consideremos la norma $\Vert$·$\Vert _1$ en $\mathbb{R}^N$ y en $\mathbb{R}^M$, entonces la norma $\Vert$·$\Vert _1$ inducida en $\mathbb{R}^{M \times N}$ es
\[ \Vert A \Vert _1 = max \; \left\lbrace \sum_{i=1}^M \vert a_{ij} \vert : j = 1,...,N \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{nprop}

Es decir, es el máximo de las sumas de los valores absolutos de cada $\textbf{columna}$.

\begin{proof} Vamos a demostrar que es $\geq$ y $\leq$, luego se dará la igualdad.\\
Probaremos primero que $ \Vert A \Vert _\infty \geq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace $\\
Sea sign(a) := $\left\{ \begin{array}{lcc}
-1 & si & a < 0 \\
1 & si & a \geq 0
\end{array}
\right.$ $\qquad$ $\forall a \in \mathbb{R}$\\
Tenemos que
\[ \left\Vert \left[ sign(a_{11}),...,sign(a_{1N}) \right] ^T \right\Vert _\infty = 1 \qquad \Rightarrow \qquad \Vert A \Vert _\infty \geq \left\Vert A \left[ sign(a_{11}),...,sign(a_{1N}) \right] ^T \right\Vert _\infty \geq \sum_{j=1}^N \vert a_{1j} \vert \]
Hacemos lo mismo con $\left[ sign(a_{i1}),...,sign(a_{iN}) \right] ^T \; \forall i = 2,...,M$ y obtenemos que
\[ \Vert A \Vert _\infty \geq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace \]
Ahora probaremos que $ \Vert A \Vert _\infty \leq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace $\\
Sea x $\in \mathbb{R}^N$ tal que $\Vert x \Vert _\infty = 1$, entonces:
\[ \Vert Ax \Vert _\infty = \left\Vert 
\begin{bmatrix} 
a_{11} &  \cdots & a_{1N} \\
\vdots & & \vdots 
\\ a_{M1} & \cdots & a_{MN} \\ \end{bmatrix} 
\begin{bmatrix}
x_1 \\
\vdots \\
x_N \\
\end{bmatrix}
\right\Vert _\infty = \left\Vert 
\begin{bmatrix}
\sum_{j=1}^N a_{1j}x_j & ,\ldots , & \sum_{j=1}^N a_{Mj}x_j 
\end{bmatrix} ^T
\right\Vert _\infty = \] \[= max \; \left\lbrace \vert \sum_{j=1}^N a_{ij}x_j \vert : i = 1,...,M \right\rbrace \leq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert \vert x_j \vert : i = 1,...,M \right\rbrace \leq \] \[ \leq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace \] 
\end{proof}

\begin{nprop}
Consideremos la norma $\Vert$·$\Vert _\infty$ en $\mathbb{R}^N$ y en $\mathbb{R}^M$, entonces la norma $\Vert$·$\Vert _\infty$ inducida en $\mathbb{R}^{M \times N}$ es
\[ \Vert A \Vert _\infty = max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,N \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{nprop}

Es decir, es el máximo de las sumas de los valores absolutos de cada $\textbf{fila}$.\\

Por lo que si ya hemos calculado alguna de estas dos últimas normas, podemos saber la otra sin tener que volver a calcular el máximo, es decir, la relación entre ambas viene en la siguiente proposición.

\begin{nprop}
$\Vert A \Vert _1 = \Vert A^T \Vert _\infty \qquad \forall A \in \mathbb{R}^{M \times N}$
\end{nprop}

Hay que tener en cuenta que $\Vert$ · $\Vert _2$ no induce en $\mathbb{R}^{M \times N}$ Frobenius.\\

Además, para las matrices $\textbf{cuadradas}$ tenemos la siguiente definición.

\begin{ndef}[Radio espectral]
Sea $A \in \mathbb{R}^{N \times N}$, denotaremos como $\textbf{radio espectral de A}$ a:
\[ \rho (A) := max \; \left\lbrace \vert \lambda \vert : \lambda \in \mathbb{C} \wedge det(A - \lambda I) = 0 \right\rbrace \]
\end{ndef}

La siguiente proposición muestra una manera más fácil de calcular la norma euclídea de un vector de $\mathbb{R}^N$, que es calculando la suma de las coordenadas al cuadrado.

\begin{nprop}
$\Vert x \Vert _2 = \sqrt{x^Tx} \qquad \forall x \in \mathbb{R}^N$
\end{nprop}

\begin{ndef}
Sea A $\in \mathbb{R}^{N \times N}$, diremos que A es $\textbf{semidefinida positiva} \Leftrightarrow x^TAx \geq 0 \qquad \forall x \in \mathbb{R}^N $
\end{ndef}

Las matrices semidefinidas positivas tienen la siguiente propiedad.

\begin{nprop}
Sea A $\in \mathbb{R}^{N \times N}$ semidefinida positiva. Si $\lambda$ es un valor propio de A $\Rightarrow \lambda \geq 0$.
\end{nprop}

\begin{proof}
Como $\lambda$ es valor propio de A $\Rightarrow \exists x \in \mathbb{R}^N : x \neq 0 \wedge Ax = \lambda x$\\
Luego\\
\[ 0 \leq x^TAx = x^T \lambda x = \lambda x^Tx = \lambda \Vert x \Vert _2^2 \]
Como x $\neq$ 0 $\Rightarrow 0 \leq \lambda$
\end{proof}

\begin{nprop}
Sea P $\in \mathbb{R}^{N \times N}$ una matriz ortogonal, entonces
\[ \left\lbrace x \in \mathbb{R}^N : \Vert x \Vert _2 = 1 \right\rbrace = \left\lbrace P^Tx : x \in \mathbb{R}^N \wedge \Vert x \Vert _2 = 1 \right\rbrace \]
\end{nprop}

\begin{proof}
Vamos a demostrar la doble inclusión, lo que dará la igualdad.\\
$\textbf{¿ $\supseteq$ ?}$\\
Sea $x \in \mathbb{R}^N : \Vert x \Vert _2 = 1 \; \Rightarrow \;$ ¿ $\Vert P^Tx \Vert _2 = 1 $ ?
\[ \Vert P^Tx \Vert _2 = \sqrt{x^TPP^Tx} = \sqrt{x^Tx} = \Vert x \Vert _2 = 1 \]
$\textbf{¿ $\subseteq$ ?}$\\
\[ 1 = \Vert x \Vert _2 = \sqrt{x^Tx} = \sqrt{x^TIx} = \sqrt{x^TPP^Tx} = \Vert P^Tx \Vert _2 \]
\end{proof}

\begin{nprop}
Si $\lambda _1,..., \lambda _N \geq 0 \Rightarrow sup \; \left\lbrace \sqrt{\sum_{i=1}^N \lambda _iy_i^2} : y \in \mathbb{R}^N \wedge \Vert y \Vert _2 = 1 \right\rbrace = \sqrt{max \; \lambda _i : i = 1,...,N} $
\end{nprop}

\begin{proof}
\end{proof}

Una manera más sencilla de calcular la norma de una matriz es la siguiente.

\begin{nprop}
Sea A $\in \mathbb{R}^{M \times N} \Rightarrow \Vert A \Vert _2 = \sqrt{\rho (A^TA)} $
\end{nprop}

\begin{ndef}[Norma matricial]
Una norma en $\mathbb{R}^{N \times N}$ se dice $\textbf{matricial}$ cuando
\[ \Vert AB \Vert \leq \Vert A \Vert \Vert B \Vert \qquad \forall A,B \in \mathbb{R}^{N \times N} \]
\end{ndef}

Hay que tener en cuenta de que no toda norma en $\mathbb{R}^{N \times N}$ es matricial, por ejemplo:\\
Vamos a utilizar la siguiente norma\\
$\Vert A \Vert := max \; \lbrace \vert a{ij} \vert : i, j = 1,...,N \rbrace \qquad \forall A \in \mathbb{R}^{N \times N}$ \\
Sean A = B = $\begin{bmatrix}
1 & 1 \\
1 & 1 \\
\end{bmatrix}$\\
Luego tenemos que
$2 = \Vert AB \Vert > \Vert A \Vert \Vert B \Vert = 1$\\
Por lo que esta norma no es matricial.\\

\begin{nprop}
Toda norma en $\mathbb{R}^{N \times N}$ inducida por una norma en $\mathbb{R}^N$ es matricial.
\end{nprop}

\begin{proof}
Sea $\Vert$ · $\Vert$ una norma en $\mathbb{R}^{N \times N}$ inducida por una norma en $\mathbb{R}^N$, entonces
\[ \Vert A \Vert := sup \; \left\lbrace \Vert Ax \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \]
Sabemos que la norma es inducida, luego se cumple que
\[ \Vert Ax \Vert \leq \Vert A \Vert \Vert x \Vert \qquad \forall x \in \mathbb{R}^N \]
Tenemos que probar que $\Vert AB \Vert \leq \Vert A \Vert \Vert B \Vert$\\
\[ \Vert AB \Vert = sup \; \left\lbrace \Vert ABx \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \leq sup \; \left\lbrace \Vert A \Vert \Vert Bx \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \leq \] \[ \leq sup \; \left\lbrace \Vert A \Vert \Vert B \Vert \Vert x \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace = \Vert A \Vert \Vert B \Vert \]
\end{proof}

\begin{nth}
$\lim_{n \rightarrow \infty} A^n = 0 \; \Leftrightarrow \; \rho (A) < 1 \qquad \forall A \in \mathbb{R}^{N \times N} $
\end{nth}

Este teorema nos deja dos importantes consecuencias.

\begin{ncor}
Sea A $\in \mathbb{R}^{N \times N}$ una matriz triangular, entonces
\[ \lim_{n \rightarrow \infty} A^n = 0 \quad \Leftrightarrow \quad max \; \lbrace \vert a_{ii} \vert < 1 : i = 1,...,N \rbrace \]
\end{ncor}

\begin{proof}
\end{proof}

\begin{ncor}
Sean N $\geq$ 1, A $\in \mathbb{R}^{N \times N}$ y $\Vert$ · $\Vert$ una norma matricial en $\mathbb{R}^{N \times N}$ tal que $\Vert$A$\Vert$ < 1, entonces $\rho$(A) < 1
\end{ncor}

Hay que tener en cuenta que no se cumple la implicación contraria, por ejemplo:\\
Sea A = $\begin{bmatrix}
0.5 & 500 \\
0 & 0.5 \\
\end{bmatrix}$ $\Rightarrow \rho (A) = 0.5 < 1$ pero $\Vert A \Vert _\infty = 500.5 \geq 1$


\subsection{Problemas bien planteados. Estabilidad}
Nos planteamos el siguiente problema:\\
Sean X e Y subconjuntos no vacíos de sendos espacios normados reales, f: X $\rightarrow$ Y una aplicación, $y_0 \in$ Y. Entonces tenemos que encontrar $x_0 \in X : f(x_0) = y_0$\\
Denotaremos a $x_0$ como la solución que resuelve el problema determinado por f y a $y_0$ los datos, es decir, son números. Si tenemos un conjunto finito de números, usaremos el vector de $\mathbb{R}^N$ o matriz, y si tenemos infinitos datos, usaremos una función.

\begin{ejemplo}
Sean $A \in \mathbb{R}^{M \times N}$ y $y \in \mathbb{R}^M$. Determinar una solución del sistema de ecuaciones lineales cuya matriz de coeficientes sea A y su vector de términos independientes sea y\\
$X = \mathbb{R}^N$, $Y = \mathbb{R}^M$, f(x) = Ax = y
\end{ejemplo}

\begin{ndef}
Un problema está $\textbf{bien planteado}$ cuando es $\textbf{unisolvente}$ y $\textbf{estable}$:
	\begin{nlist}
	\item $\exists ! x_0 \in X : f(x_0) = y_0$.
	\item $x_0$ depende continuamente de los datos $y_0$.
	\end{nlist}
\end{ndef}

En el siguiente ejemplo veremos un problema mal planteado.

\begin{ejemplo}
Sean X := $\mathbb{R}$, Y := $\mathbb{R}_+ \;$ y $\; f(x) := \vert x \vert , \; \forall x \in X$\\
Observamos que este problema no es unisolvente, ya que si $y_0$ = 1 (lo mismo vale $\forall y_0 > 0$) tenemos que f(-1) = 1 = f(1)
\end{ejemplo}

\begin{ndef}[Resolvente]
Denotaremos a la función g como la $\textbf{resolvente}$ de f si g es la inversa de f, para todo y $\in$ Y unisolvente.
\end{ndef}

\begin{ejemplo}
Sean X := $\mathbb{R}$, Y := $\mathbb{R}_+ \;$ y $\; f(x) := e^x , \; \forall x \in X$\\
Entonces este problema es unisolvente, luego tiene resolvente: g(y) = log y, para todo y $\in$ Y.
\end{ejemplo}

Podemos ver la estabilidad de un problema intuitivamente, es decir, a pequeñas perturbaciones de los datos $y_0$ corresponden pequeñas perturbaciones de la solución $x_0$.

\begin{ejemplo}
Consideremos el problema X := [-1,1] =: Y, $\;$
$
f(x) := \left\{ \begin{array}{lcc}
x & si & -1 < x < 1 \\
\\ -x & si & x = \pm 1
\end{array}
\right.
$.\\
Entonces este problema es unisolvente, ya que la función f :  [-1,1] $\rightarrow$ [-1,1] es biyectiva y g = f.\\
Pero para que esté bien planteado, tiene que ser, además, estable. Veamos que no es estable, ya que a pequeñas perturbaciones del dato $y_0$ = -1 (lo mismo con $y_0$ = 1) no corresponden pequeñas perturbaciones de $x_0$ = 1.\\
Sea $y_n := -1 + \frac{1}{n} \rightarrow y_0 = -1$, entonces $g(y_n) = -1 + \frac{1}{n} \rightarrow -1$, ya que $y_n = g(y_n)$.\\
Luego $\vert -1 - x_0 \vert = 2$, es decir, las perturbaciones de $x_0$ son muy grandes.
\end{ejemplo}

Ahora nos planteamos la siguiente cuestión: ¿La estabilidad del problema y la continuidad de la resolvente g tienen algo que ver? Veamos en el siguiente ejemplo que no siempre es así.

\begin{ejemplo}
Consideremos el problema $f : \mathbb{R}_+ \rightarrow \mathbb{R}_+, f(x) := \left( \frac{x}{10} \right) ^10 \; \forall x \geq 0$.\\
Tenemos que este problema es unisolvente y su resolvente es $g : \mathbb{R}_+ \rightarrow \mathbb{R}_+, g(y) = 10y^{\frac{1}{10}} \; \forall y \geq 0$.\\
Tenemos que g es continua, luego pequeños cambios de los datos $y \in \mathbb{R}_+$ nos llevan a cambios cercanos de las correspondientes soluciones $x \in \mathbb{R}_+$ pero no controlados, se aproximan a una velocidad diferente. Vamos a comprobarlo.\\
Tenemos que $y_0 = 0 = x_0$. Sea y un dato próximo a $y_0$, por ejemplo y = $10^{-10}$. Entonces la solución correspondiente es $x = g(y) = 10y^{\frac{1}{10}} = 10 \cdot \left( 10^{-10} \right) ^{\frac{1}{10}} = \frac{10}{10} = 1$. Por lo que tenemos lo siguiente:\\
$\vert y - y_0 \vert = 10^{-10}$, $\vert x - x_0 \vert = 1$.\\
Es decir, el número $10^{-10}$ indica la velocidad con la que se mueven los datos y, en comparacion con el número 1, que indica que los datos x se mueven mucho más rápido, lo que nos lleva a que, aunque g sea continua, la velocidad de aproximación es diferente, luego no hay estabilidad.
\end{ejemplo}

Luego podríamos decir que la estabilidad es una condición que fuerce un control de los valores de las soluciones en función de los datos, de forma que pequeñas perturbaciones de $y_0$ generen perturbaciones pequeñas y controladas de $x_0$.

\begin{ndef}
Sean X e Y subconjuntos no vacíos de sendos espacios normados, $g : Y \rightarrow X$ una aplicación e $y_0 \in Y$. Diremos que g es $\textbf{estable}$ en $y_0$ cuando
 \[ \exists \delta , M > 0 : sup \; \left\lbrace \frac{ \Vert g(y) - g(y_0) \Vert }{ \Vert y - y_0 \Vert } : y \in Y \wedge 0 < \Vert y - y_0 \Vert < \delta \right\rbrace < M \]
y que g es $\textbf{estable}$ si lo es en todos y cada uno de los elementos de Y.
\end{ndef}

\begin{ndef}
Un problema es $\textbf{estable en $y_0 \in$ Y}$ si su resolvente $g : Y \rightarrow X$ lo es en dicho punto, y es $\textbf{estable}$ si lo es en cualquier dato de Y.
\end{ndef}

Habrá mejor comportamiento cuanto más pequeño sea M.

\begin{nprop}
g es estable en $y_0 \Rightarrow$ g es continua en $y_0$ 
\end{nprop}

Veamos que la implicación $\nLeftarrow$ se cumple al tomar la resolvente del ejemplo anterior en 0, o si se quiere, la función raíz cuadrada en 0.\\

\begin{nprop}
Toda función real de variable real de clase $C^1$ es estable.
\end{nprop}

Para demostrar esta proposición hay que usar el Teorema del Valor Medio. Además, el recíproco no es cierto.\\
Sea f una función de variable real de clase $C^1$, vamos a medir su estabilidad en $x_0 \in \mathbb{R}$.\\
Si $x_0f(x_0) \neq 0$, entonces el cociente entre el error relativo cometido cerca de $f(x_0)$ y el error relativo de $x_0$ es:
\[ \left\vert \frac{\frac{f(x) - f(x_0)}{f(x_0)}}{\frac{x - x_0}{x_0}} \right\vert = \left\vert \frac{f(x) - f(x_0)}{x - x_0} \right\vert \left\vert \frac{x_0}{f(x_0)} \right\vert \]
Si $x_0f(x_0) = 0$, entonces los errores absolutos cerca de $f(x_0)$ e $x_0$ es:
\[ \left\vert \frac{f(x) - f(x_0)}{x - x_0} \right\vert \]

De forma precisa:

\begin{ndef}
Dada una función $f \in C^1(\mathbb{R})$ y un punto $x_0 \in \mathbb{R}$, el $\textbf{condicionamiento relativo}$ de f en $x_0$ viene dado por
\[ c(f, x_0) := \left\vert \frac{f'(x_0)x_0}{f(x_0)} \right\vert \]
siempre que $x_0f(x_0) \neq 0$.\\
En caso contrario, el $\textbf{condicionamiento absoluto}$ de f en $x_0$ es
\[ C(f, x_0) := \vert f'(x_0) \vert \]
\end{ndef}

Ídem en funciones reales de variable real definidas en intervalos de $\mathbb{R}$ y de clase $C^1$.\\
Para calcular el condicionamiento en funciones de varias variables, usamos lo siguiente:
\[ c(f,x_0) := \frac{\left\Vert \frac{\partial f}{\partial x} (x_0) \right\Vert \Vert x_0 \Vert}{\Vert f(x_0) \Vert} \]
\[ C(f,x_0) := \left\Vert \frac{\partial f}{\partial x} (x_0) \right\Vert \]

\begin{ndef}
Si en un problema la resolvente s de clase $C^1$ e $y_0 \in Y$, el $\textbf{condicionamiento relativo}$ o $\textbf{absoluto}$ del problema son los de su resolvente en dicho punto.
\end{ndef}

\begin{nota}
Aunque se trata de un concepto bastante ambiguo, suele decirse que una aplicación (o un problema) está $\textbf{bien condicionado}$ en un punto si su condicionamiento es pequeño y en caso contrario, $\textbf{mal condicionado}$.
\end{nota}

\begin{ejemplo}
Sea $f : (0,1) \rightarrow (0, \pi / 2), \; f(x) := arcsen(x)$.\\
Veamos si está bien planteado.\\
La resolvente es $g : (0, \pi / 2) \rightarrow (0,1), \; g(y) := sen(y)$. Luego está bien planteado.\\
Veamos si está bien condicionado.\\ Para ello calculamos el condicionamiento relativo en $y_0 \in (0, \pi / 2)$.\\
\[ c(g,y_0) = \frac{\vert g'(y_0)y_0 \vert }{\vert g(y_0) \vert} = y_0 \frac{cos(y_0)}{sen(y_0)} \]
Como $c(g,y_0) \in (0,1) \Rightarrow$ está bien condicionado.
\end{ejemplo}

\begin{ejemplo}
Sea $A \in \mathbb{R}^{N \times N}$ regular e $y \in \mathbb{R}$, encuentra $x \in \mathbb{R}^N$ : f(x) = y.\\
Sea f(x) = Ax, como A es regular, es unisolvente el problema, luego tiene resolvente $g : \mathbb{R}^N \rightarrow \mathbb{R}^N$, $g(y) := A^{-1}y, \; \; \forall y \in \mathbb{R}^N$\\
Además, el problema es estable en todo $y_0 \in \mathbb{R}^N$, ya que:
\[ \Vert g(y) - g(y_0) \Vert _\infty \leq \Vert A^{-1} \Vert _\infty \Vert y - y_0 \Vert _\infty \]
Ahora calcularemos el condicionamiento relativo.
\[ c(g, y_0) = \frac{\Vert \frac{\partial g}{\partial y} (y_0) \Vert \Vert y_0 \Vert }{\Vert g(y_0)} = \frac{\Vert A^{-1} \Vert \Vert y_0 \Vert }{\Vert A^{-1} y_0 \Vert } = \frac{\Vert A^{-1} \Vert \Vert y_0 \Vert }{\Vert x_0 \Vert } \] 
Luego $Ax_0 = y_0$
\end{ejemplo}

Si $\Vert A^{-1} \Vert$ es grande, $x_0$ e $y_0$ son del mismo orden, entonces el condicionamiento será grande. Observémoslo en el siguiente ejemplo.

\begin{ejemplo}
Sea $A = \begin{bmatrix}
1 & 1 \\
1 & 0.999 \\
\end{bmatrix}$,
un dato $y_0 = \begin{bmatrix}
2 \\
1.999 \\
\end{bmatrix}$
y su solución $x_0 = \begin{bmatrix}
1 \\
1 \\
\end{bmatrix}$\\

Veamos que hay un mal condicionamiento, ya que el condicionamiento es demasiado grande en comparación con el resto. Para ello, calculamos antes la inversa de la matriz A y la norma del máximo de esta, de la solución $x_0$ y del dato $y_0$.

\[ A^{-1} = \begin{bmatrix}
-999 & 1000 \\
1000 & -1000 \\
\end{bmatrix} \]
\[ \Vert A^{-1} \Vert _\infty = 2000, \; \; \Vert x_0 \Vert _\infty = 1, \; \; \Vert y_0 \Vert _\infty = 2 \]
Luego el condicionamiento se calculará con la expresión obtenida anteriormente usando la norma infinito.
\[ c(g, y_0) = \frac{\Vert A^{-1} \Vert _\infty \Vert y_0 \Vert _\infty }{\Vert x_0 \Vert _\infty } = 4000 \]
Como hemos dicho, es un condicionamiento demasiado grande y nos preguntamos ¿cuál es el problema? Pues que al calcular las soluciones de otros datos próximos al inicial, las soluciones obtenidas no son próximas a la solución inicial. Comprobémoslo.\\
Sea y un dato próximo a $y_0$, por ejemplo $y = \begin{bmatrix}
2 \\
1.998 \\
\end{bmatrix}$.
Es próximo, ya que $\Vert y - y_0 \Vert _\infty = 0.001$.\\
Su solución es $x = A^{-1}y = \begin{bmatrix}
0 \\
2 \\
\end{bmatrix}$. No es próxima a la solución inicial, ya que $\Vert x - x_0 \Vert _\infty = 1$\\
La interpretación geométrica de este resultado es: las columnas de A forman base de $\mathbb{R}^{2 \times 2}$, calculamos las coordenadas de y en $y_0$ en dicha base. Un pequeño cambio entre y e $y_0$ genera un cambio grande de coordenadas por ser los vectores de la base casi paralelos.
\end{ejemplo}

Ahora nos planteamos la siguiente pregunta: ¿está globalmente acotado el condicionamiento relativo de un sistema de cuaciones lineales? La respuesta es afirmativa y nos da una idea de cómo de estable es.\\

La siguiente definición será útil para calcular el condicionamiento de una matriz cuando la norma es inducida.

\begin{ndef}
Si $\Vert \cdot \Vert$ denota la norma matricial en $\mathbb{R}^{N \times N}$ inducida por una norma en $\mathbb{R}^N$, que notaremos igualmente como $\Vert \cdot \Vert$, entonces definimos el $\textbf{condicionamiento}$ c(A) de una matriz regular A $\in \mathbb{R}^{N \times N}$ como
\[ c(A) := \Vert A^{-1} \Vert \Vert A \Vert \]
\end{ndef}

Además,
\[ sup \; \left\lbrace c(g, y_0) : y_0 \neq 0 \right\rbrace = sup \; \left\lbrace \frac{\Vert A^{-1} \Vert \Vert y_0 \Vert}{\Vert A^{-1}y_0 \Vert} : y_0 \neq 0 \right\rbrace = sup \; \left\lbrace \frac{\Vert A^{-1} \Vert \Vert Ax_0 \Vert}{\Vert x_0 \Vert} : x_0 \neq 0 \right\rbrace = \Vert A^{-1} \Vert \Vert A \Vert \]

Lo que nos lleva a que si tenemos un mal condicionamiento del sistema, el condicionamiento de la matriz de coeficientes es grande, y viceversa.\\
También tenemos que $c(A) \geq 1$.\\
Si retomamos el ejemplo anterior, usando la norma del máximo en $\mathbb{R}^2$ y la correspondiente norma matricial inducida, obtenemos que c(A) = 4000.

\subsection{Algoritmos. Algoritmo PageRank de Google}

\begin{ndef}[Algoritmo]
Procedimiento que describe de forma precisa, y siempre mediante un número finito de operaciones aritméticas y lógicas elementales, la resolución de un problema.
\end{ndef}

El algoritmo recoge las instrucciones que permiten al ejecutor del mismo resolver completamente el problema. El ejecutor suele ser un ordenador y, de hecho, en la mayoría de los casos, no puede ser una persona.

\begin{ndef}[Análisis Numérico]
Se ocupa de diseñar algoritmos que permitan la resolución efectiva de problemas bien planteados y que involucran números reales.
\end{ndef}

\begin{ndef}[Complejidad de un algoritmo]
Medida del tiempo de ejecución y que suele expresarse en términos de un parámetro asociado al problema.
\end{ndef}

Al desarrollar y analizar un algoritmo obtenemos precisión, estabilidad y efectos de la representación finita de los números reales.

El Algoritmo PageRank de Google mide la relevancia de las páginas web (dominios...) con enlaces en común. Por lo que aquí nuestro problema es calcular la relevancia de una página (P), teniendo en cuenta el número de enlaces de otras páginas a (P) y la importancia de las páginas que establecen enlace con (P).

Nuestro modelo es:\\
Denotaremos al conjunto (finito) de páginas web como 1, 2, ..., $x_N$.\\
Denotaremos a las páginas como 1, 2, ..., i.\\
La relevancia de la página i se representa por $x_i \geq 0$.\\
La página i será más importante que la página j si $x_i > x_j$.

\begin{ejemplo}
%Insertar imagen
La relevancia de cada página será la suma de los cocientes entre la relevancia de la página que entra y los enlaces que salen de esta.
\[ x_1 = \frac{x_3}{3}, \; \; x_2 = \frac{x_1}{3} + \frac{x_3}{3} + \frac{x_4}{2}, \; \; x_3 = \frac{x_1}{3} + \frac{x_1}{3} + \frac{x_4}{2}, \; \; x_4 = \frac{x_1}{3} + x_2 + \frac{x_3}{3} \]

Por lo que siempre obtendremos un sistema compatible indeterminado con un parámetro.\\

Si tomamos $x_4$ como parámetro y le asignamos el valor 10, tenemos lo siguiente:
\[ x_1 = 1.875, \; \; x_2 = 7.5, \; \; x_3 = 5.625, \; \; x_4 = 10 \]
\end{ejemplo}

\section{Errores de redondeo. Iteradores}
Las principales fuentes de error suelen ser equivocarse al elegir el modelo de un problema, la medida de los datos experimentales, error al truncar o al redondear y consecuentemente, al operar, se produce una propagación del error.

\subsection{Sistema posicional y números máquina}
Los ordenadores trabajan con un subconjunto finito de números reales, los $\textbf{números máquina}$, subconjunto que depende de las especificaciones del ordenador.\\

$\textbf{SISTEMAS POSICIONALES DE NUMERACIÓN}$\\
Sea $b \in \mathbb{N}$ la base binaria o decimal, según el estándar ISO/IEC/IEEE60559:2011 del IEEE (Institute of Electrical and Electronic Engineers, www.ieee.org). Sea $s \in$ {0,1} el signo, N,M $\in \mathbb{N} \cup$ {0} y sea $x_k$ la cifra en la posición k tal que $0 \leq x_k < b, \; \; \forall k = -M, ..., N$.\\
La $\textbf{representación posicional}$ de un número real es:
\[ x = (-1)^s \sum_{n=-M}^N x_nb^n \]
\[ x_b := (-1)^s \cdot (x_N...x_1x_0.x_{-1}x_{-2}...x_{-M})_b \]
Además, denominaremos al punto entre $x_0$ y $x_{-1}$ como $\textbf{punto binario}$ o $\textbf{punto decimal}$.

\begin{ejemplo}
Sea $x_{10} = x = 101.11$, su representación posicional es x = 1·$10^2$ + 1·$10^0$ + 1·$10^{-1}$ + 1·$10^{-2}$.\\
Sea $y_2 = (101.11)_2$, su representación posicional es y = 1·$2^2$ + 1·$2^0$ + 1·$2^{-1}$ + 1·$2^{-2}$.
\end{ejemplo}

Variando $M, N \in \mathbb{N} \cup \lbrace 0 \rbrace $ tenemos un subconjunto denso de $\mathbb{R}$. Además, la serie geométrica de razón menor que 1 converge. Se puede extender a infinito:
\[ x = (-1)^s \sum_{- \infty}^N x_nb^n \]
\[ x_b := (-1)^s \cdot (x_N...x_1x_0.x_{-1}x_{-2}...)_b \]

El siguiente ejemplo aprenderemos representar en forma posicional infinita a partir de otra infinita.

\begin{ejemplo}
\[ (0.\stackrel{\large \frown}{001}) _2 = \sum_{n=1}^{\infty} \left( 2^{-3} \right) ^n = \frac{1}{7} = \stackrel{\Large \frown}{0.142857} \]
Para calcular la convergencia de esta serie, como la razón es menor que 1, hemos usado lo siguiente:
\[ \alpha \neq 1, \; n > k \geq 0 \; \; \Rightarrow \sum_{j=k}^n \alpha ^j = \frac{\alpha ^k - \alpha ^{n+1}}{1 - \alpha} \]
En particular, si \[ \vert \alpha \vert < 1 \; \; \Rightarrow  \sum_{j=k}^\infty \alpha ^k = \frac{\alpha ^k}{1 - \alpha} \]
\end{ejemplo}


$\textbf{NÚMEROS MÁQUINA}$

Estos números los utiliza el ordenador. Denotamos a la base como b y a las posiciones de memoria como N.

\begin{ndef}[Representación con punto fijo]
Sea $k \in \mathbb{N}$ fijo, N - k -1 dígitos enteros, k dígitos tras el punto $a_n$ tal que $0 \leq a_n \leq b - 1$. Entonces las N posiciones de memoria son: N = signo + cifras significativas = 1 + N - 1. La representación con punto fijo es la siguiente:
\[ (-1)^s b^{-k} \cdot \sum_{n=0}^{N-2}a_nb^n \; \sim \; (-1)^s \cdot (a_{N-2}...a_k.a_{k-1}...a_0)_b \]
\end{ndef}

\begin{ndef}[Representación con número flotante]
Sea $t \in \mathbb{N}$ el número máximo de dígitos o cifras significativas $a_n$ tales que $0 \leq a_n \leq b - 1$, sea m = $a_1...a_t$ la mantisa tal que $0 \leq m \leq b^t - 1$, sea e $\in \mathbb{Z}$ el exponente, tal que $L \leq e \leq U$ donde L, U $\in \mathbb{Z}$, $L \leq U$. Entonces las N posiciones de memoria son N = signo + cifras significativas + dígitos del exponente = 1 + t + N - t - 1. La representación con punto flotante es:
\[ (-1)^sb^e \cdot \sum_{n=1}^t a_nb^{-n} \; \sim \; (-1)^s \cdot (0.a_1...a_t) \cdot b^e = (-1)^s \cdot m \cdot b^{e-t} \]
\end{ndef}

\begin{ejemplo}
Sea x = -3.4567 y la base b = 10, vamos a representarlo de las dos maneras.
	\begin{nlist}
	\item Representación con punto fijo con k = 4 y N = 6. Luego $x = (-1) \cdot 10^{-4} \cdot (3 \cdot 10^4 + 4 \cdot 10^3 + 5 \cdot 10^2 + 6 \cdot 10 + 7 \cdot 10^0)·$ = (-1)(3.4567)
	\item Representación con punto flotante con t = 6 y e = 2. Luego $x = (-1) \cdot 10^2 \cdot (0 \cdot 10^{-1} + 3 \cdot 10^{-2} + 4 \cdot 10^{-3} + 5 \cdot 10^{-4} + 6 \cdot 10^{-5} + 7 \cdot 10^{-6}) = (-1)(0.034567) \cdot 10^2$
	\end{nlist}
\end{ejemplo}

Usualmente hay dos representaciones de punto flotante: la precisión simple y la doble.\\
Si un número no está normalizado, es decir, la cifra significativa principal $a_1$ no es 0, entonces ese número tendrá varias representaciones. Veámoslo en el siguiente ejemplo. 

\begin{ejemplo}
Sean b = 2, t = 3, L = 1, U = 3 y punto flotante para x = 1, entonces:
\[ (0.100) \cdot 2^1 = (0.010) \cdot 2^2 = (0.001) \cdot 2^3 \]
\end{ejemplo}

Para evitar este problema, usaremos la representación normalizada que viene definida a continuación.

\begin{ndef}[Notación del sistema normalizado de punto flotante]
\[ \mathbb{F} (b,t,L,U) := \lbrace 0 \rbrace \cup \left\lbrace (-1)^sb^e \sum_{n=1}^t a_nb^{-n} : s = 0,1, a_1 \neq 0, 0 \leq a_1,...,a_t \leq b - 1, L \leq e \leq U \right\rbrace \]
\end{ndef}

\begin{nprop}
Sean t $\in \mathbb{N}$, L, U $\in \mathbb{Z}$ con L $\leq$ U y x $\in \mathbb{F} (b,t,L,U)$. Entonces:
	\begin{nlist}
	\item -x $\in \mathbb{F} (b,t,L,U)$.
	\item $b^{L-1} \leq \vert x \vert \leq b^U(1 - b^{-t})$.
	\item card$( \mathbb{F} (b,t,L,U)) = 2(b-1)b^{t-1}(U-L+1)+1$.
	\end{nlist}
\end{nprop}

Veamos un importante ejemplo en el que se representan todos los números positivos de un sistema de punto flotante normalizado.

\begin{ejemplo}
Sea $\mathbb{F} (2,4,-1,1)$, entonces los números estrictamente positivos de ese sistema de punto flotante son:

\[ \begin{array}{llll}
(0.1111) \cdot 2 = \frac{15}{8} & (0.1110) \cdot 2 = \frac{7}{4} & (0.1101) \cdot 2 = \frac{13}{8} & (0.1100) \cdot 2 = \frac{3}{2} \\
\\ (0.1011) \cdot 2 = \frac{11}{8} & (0.1010) \cdot 2 = \frac{5}{4} & (0.1001) \cdot 2 = \frac{9}{8} & (0.1000) \cdot 2 = 1 \\
\\ (0.1111) \cdot 2^0 = \frac{15}{16} & (0.1110) \cdot 2^0 = \frac{7}{8} & (0.1101) \cdot 2^0 = \frac{13}{16} & (0.1100) \cdot 2^0 = \frac{3}{4}\\
\\ (0.1011) \cdot 2^0 = \frac{11}{16} & (0.1010) \cdot 2^0 = \frac{5}{8} & (0.1001) \cdot 2^0 = \frac{9}{16} & (0.1000) \cdot 2^0 = \frac{1}{2}\\
\\ (0.1111) \cdot 2^{-1} = \frac{15}{32} & (0.1110) \cdot 2^{-1} = \frac{7}{16} & (0.1101) \cdot 2^{-1} = \frac{13}{32} & (0.1100) \cdot 2^{-1} = \frac{3}{8}\\
\\ (0.1011) \cdot 2^{-1} = \frac{11}{32} & (0.1010) \cdot 2^{-1} = \frac{5}{16} & (0.1001) \cdot 2^{-1} = \frac{9}{32} & (0.1000) \cdot 2^{-1} = \frac{1}{4}
\end{array} \]
Además, podemos calcular lo siguiente:\\
Número de elementos de $\mathbb{F}$(2,4,-1,1) (positivos, negativos y cero):
\[ 2(b-1)b^{t-1}(U-L+1) + 1 = 49 \]
Valores mínimo y máximo (positivos):
\[ b^{L-1} = \frac{1}{4}, \; \; b^U(1-b^{-t}) = \frac{15}{8} \]
\end{ejemplo}

El sistema $\mathbb{F}$(b,t,L,U) no se distribuye uniformemente, aunque sí por bloques.

\begin{ndef}[Épsilon máquina]
Para un sistema de punto flotante $\mathbb{F}$ (b,t,L,U) con $L \leq 1 \leq U$, el $\textbf{épsilon máquina}$, que se escribe como $\varepsilon _M$, es la distancia entre el menor número de $\mathbb{F}$ (b,t,L,U) mayor que 1 y la propia unidad, es decir,
\[ \varepsilon _M := b^{1-t} \]
\end{ndef}

Como $L \leq 1 \leq U \Rightarrow 1 \in \mathbb{F} (b,t,L,U)$ 

\subsection{Redondeo en sistemas de punto flotante y su aritmética}
En este apartado vamos a trabajar sobre el contexto de los números máquina representados en el sistema de punto flotante $\mathbb{F}$(b,t,L,U). Primero, tengamos en cuenta lo siguiente:
	\begin{nlist}
	\item $\mathbb{F}$(b,t,L,U) $\neq \mathbb{R}$.
	\item El resultado de operar con dos números de $\mathbb{F}$ (b,t,L,U) no queda dentro de dicho sistema necesariamente, por ejemplo:
	\[ \frac{1}{4}, \frac{9}{32} \in \mathbb{F} (2,4,-1,1), \; \; pero \; \frac{1}{4} + \frac{9}{32} = \frac{17}{32} \notin \mathbb{F} (2,4,-1,1) \]
	\end{nlist}

\begin{ndef}[Truncatura]
Fijado un sistema de punto flotante concreto $\mathbb{F}$(b,t,L,U) (al que no se hará referencia si no hay lugar a ambigüedad) si x $\in \mathbb{R}$ es el número real 
\[ x =(-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \]
entonces su $\textbf{truncatura}$ (en dicho sistema) es un número de $\mathbb{F}$(b,t,L,U)
\[ tr(x) := (-1)^s \cdot (0.a_1...a_t) \cdot b^e \]
\end{ndef}

\begin{ejemplo}
Sea el sistema de punto flotante $\mathbb{F}$(2,4,-1,1) y el número real x = 1.6875, calcularemos su truncatura. Primero lo pasaremos a la base indicada:
\[ 1.6875 = (0.11011) \cdot 2 \]
Ahora calculamos la truncatura:
\[ tr(1.6875) = (0.1101) \cdot 2 = \frac{13}{8} = 1.625 \]
Es decir, hemos quitado la última cifra, ya que en el número había 5 cifras y según el sistema, el número de cifras es 4. Por último lo hemos pasado a base decimal.
\end{ejemplo}

\begin{ndef}
Para un sistema de punto flotante  $\mathbb{F}$(b,t,L,U), el $\textbf{redondeo}$ del número real
\[ x = (-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \]
es el número de $\mathbb{F}$(b,t,L,U)
\[ rd(x) = tr \left( x + (-1)^s \frac{b}{2} \frac{b^e}{b^{t+1}} \right) \]
\end{ndef}

El redondeo se puede calcular de otra manera.

\begin{nprop}
Para un sistema de punto flotante  $\mathbb{F}$(b,t,L,U), el $\textbf{redondeo}$ del número real
\[ x = (-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \]
es el número de $\mathbb{F}$(b,t,L,U)
\[ rd(x) := (-1)^s \cdot (0.a_1...a_{t-1}r_t) \cdot b^e \]
donde
\[ r_t := \left\{ \begin{array}{lcc}
a_t & si & a_{t+1} < \frac{b}{2}\\
\\ a_t +1 & si & a_{t+1} \geq \frac{b}{2}
\end{array} 
\right. \]
\end{nprop}

\begin{ejemplo}
Sea el sistema de punto flotante  $\mathbb{F}$(2,4,-1,1) y el número real 1.6875, vamos a calcular su redondeo.
	Como la base es 2, vamos a pasarlo a esa base: $1.6875 = (0.11011) \cdot 2$
	Para redondearlo, lo podemos hacer de dos formas.
	\begin{nlist}
	\item Directamente con 5ª cifra tras el punto (t = 4):
	\[ rd(1.6875) = (0.1110) \cdot 2 = \frac{7}{4} = 1.75 \]
	\item Con la definición:
	\[ rd(1.6875) = tr \left( x +(-1)^0 \frac{2}{2} \frac{2}{b^5} \right) = tr ((0.11011) \cdot 2 + (0.00001) \cdot 2) = tr((0.11100) \cdot 2) = (0.1110) \cdot 2 = 1.75 \]
	\end{nlist}
\end{ejemplo}

Los errores de redondeo o truncatura están acotados, se muestran en la siguiente propiedad.

\begin{nprop}
Consideremos el sistema de punto flotante $\mathbb{F}(b,t,L,U)$, con $L \leq e \leq U$ y sea $x = (-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \; \in \mathbb{R}$. Entonces:
	\begin{nlist}
	\item $ \vert x - tr(x) \vert \leq b^{e-t} $
	\item $ \frac{ \vert x - tr(x) \vert }{ \vert x \vert } \leq \varepsilon _M $
	\item $ \vert x - rd(x) \vert \leq \frac{1}{2} b^{e-t}$
	\item $ \frac{\vert x - rd(x) \vert }{\vert x \vert } \leq \frac{\varepsilon _M}{2} $
	\end{nlist}
\end{nprop}

\begin{proof}
$\newline$
	\begin{nlist}
	\item 
	\[ \vert x - tr(x) \vert = b^e \left\vert \sum_{n=t+1}^{\infty} a_nb^{-n} \right\vert \leq b^e(b-1) \sum_{n=t+1}^{\infty} b^{-n} = b^{e-t} \]
	La primera igualdad se da, ya que al restar x y tr(x) el factor común es $b^e$ y se eliminan los términos del 1 hasta la t. En la segunda desigualdad hemos acotado por la cifra más grande. Finalmente, hemos calculado la suma.
	\item Usando (i) y que $a_1 \geq 1$ tenemos que 
	\[ \frac{\vert x - tr(x) \vert}{\vert x \vert} \leq \frac{b^{e-t}}{b^e \sum_{n=1}^{\infty} a_nb^{-n}} \leq \frac{b^{e-t}}{b^e \frac{1}{b}} = b^{1-t} = \varepsilon _M \]
	\item 
	\[ \vert x - rd(x) \vert = b^e \vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert \]
	 ¿$ \vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert \leq \frac{b^{-t}}{2}$ ?
		\item[•] Si $a_{t+1} < \frac{b}{2} \; y \; a_t = r_t$:
		\[ \vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert = (0.0...0a_{t+1}...) \leq (0.0...0 \frac{b}{2}) = \frac{b^{-t}}{2} \]
		\item[•] Si $a_{t+1} \geq \frac{b}{2} \; y \; a_t + 1 = r_t$:
		\[ vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert = \frac{1}{b^t} - \left( \frac{a_{t+1}}{b^{t+1}} + ... \right) \leq \frac{1}{b^t} - \frac{b}{2b^{t+1}} = \frac{b^{-t}}{2} \]
	\item \[ a_1 \geq 1 \Rightarrow \vert x \vert \geq b^eb^{-1} \Rightarrow _{(iii)} \frac{\vert x - rd(x) \vert }{\vert x \vert} \leq \frac{\frac{1}{2}b^{e-t}}{b^eb^{-1}} = \frac{1}{2}b^{1-t} = \frac{\varepsilon _M}{2} \]
	\end{nlist}	
\end{proof}

\begin{ndef}
La cota que aparece en el error relativo del redondeo recibe el nombre de $\textbf{precisión máquina}$ (o $\textbf{unidad de redondeo}$) y se denota por u, es decir,
\[ u := \frac{1}{2}b^{1-t} = \frac{1}{2} \varepsilon _M \]
\end{ndef}

\begin{ncor}
Dados $\mathbb{F} (b,t,L,U)$ y $x \in \mathbb{R}$, con $b^{L-1} \leq \vert x \vert \leq b^U(1 - b^{-t})$, se tiene que
\[ rd(x) = (1 + \mu )x \]
para cierto $\mu \in \mathbb{R}$ tal que $\vert \mu \vert \leq u$
\end{ncor}

\begin{proof}
\[ \vert x - rd(x) \vert \leq u \vert x \vert \; \Leftrightarrow \; x - \vert x \vert u \leq rd(x) \leq x + \vert x \vert u \]
esto es,
\[ rd(x) = x +\kappa \vert x \vert u \]
con $\vert \kappa \vert \leq 1$, es decir,
\[ rd(x) = (1 + \mu )x \]
para cierto $\mu \in \mathbb{R}$ tal que $\vert \mu \vert \leq u$
\end{proof}

Las operaciones con números máquina en $\mathbb{F} (b,t,L,U)$ no son necesariamente internas.

\begin{ndef}
Sea $ \bullet : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ una operación, definimos la $\textbf{operación máquina}$ como
\[ \bullet _M : \mathbb{F} (b,t,L,U) \times \mathbb{F} (b,t,L,U) \rightarrow \mathbb{F} (b,t,L,U) \]
\[ \bullet _M (x,y) := rd(x \bullet y), \; \; (x,y \in \mathbb{F} (b,t,L,U)) \]
\end{ndef}

\begin{ncor}
\[ x,y \in \mathbb{F} (b,t,L,U) \Rightarrow \bullet _M (x,y) = (1 + \mu ) x \bullet y \]
para cierto $\mu \in \mathbb{R}$ tal que $\vert \mu \vert \leq u$
\end{ncor}

Aunque una única operación genere un error pequeño, una sucesión finita de operaciones es susceptible de producir la llamada $\textbf{propagación del error}$, que puede ser considerable. Veámoslo en el siguiente ejemplo.

\begin{ejemplo}
Sea la sucesión $x_n := \int_0^1 x^ne^x dx$, vamos a integrar por partes para obtener una expresión más sencilla.
\[ \int_0^1 x^ne^x dx = x^ne^x ]_0^1 - n \int_0^1 x^{n-1}e^x dx = e - n \int_0^1 x^{n-1}e^x dx \]
Por lo que $x_n = e - nx_{n-1}$.\\
Si $x \in [0,1] \Rightarrow x^ne^x \leq x^{n-1}e^x$.\\
Tenemos que la sucesión $\lbrace x_n \rbrace _{n \geq 0}$ es decreciente y positiva, luego converge a un número $\ell$. Calculamos ese número:
\[ x_{n-1} = \frac{e - x_n}{n} \Rightarrow \ell = 0 \]
Redondeando en el sistema de punto flotante b = 10 y t = 7 obtenemos que:
\[ x_{15} = (0.1604004) \]
\[ x_0 = (0.1718281828) \cdot 10 \]
\[ x_{15} = (0.6004419078) \cdot 10^3 \]
Lo que produce una propagación del error.\\
Ahora vamos a calcular el condicionamiento de una sucesión de funciones directamente relacionadas con la sucesión $\lbrace x_n \rbrace _{n \geq 0}$. Primero tenemos que calcular usando la recurrencia $x_n$, siendo $x_n = f_n (x_0)$ donde $f_n : \mathbb{R} \rightarrow \mathbb{R}$
\[ x_1 = e - x_0 \]
\[ x_2 = e - 2(e - x_0) = -e + 2x_0 \]
\[ x_3 = e - 3(-e + 2x_0) = 4e - 6x_0 \]
Por inducción, para $n \geq 1$:
\[ (-1)^n(n!x_0 - \alpha _n e) \]
con $\alpha _n \in \mathbb{N}$ independiente de $x_0$ (es un valor irrelevante para calcular el condicionamiento).\\
Tenemos que:
\[ x_n = f_n (x_0) \]
\[ f_n (x) = (-1)^n(n!x - \alpha _ne), \; \; (x \in \mathbb{R}) \]
Ahora ya podemos calcular el condicionamiento de $f_n$ en $x_0$:
\[ c(f_n,x_0) = \frac{n!x_0}{x_n} \geq \frac{n!x_0}{x_0} = n! \]
\end{ejemplo}

A continuación vamos a estudiar las operaciones aritméticas elementales y los errores debidos que se producen en estas debidos a truncaturas, redondeos o a cualquier otra circunstancia.\\
Sea x el dato, $\mu _x$ el error relativo para x y (1 + $\mu _x$)x su valor aproximado.
	\begin{nlist}
	\item Suma (igual para la resta).\\
	Sean los datos $x, y \in \mathbb{R}$ tales que $x + y \neq 0$ y sus valores aproximados (1 + $\mu _x$)x, (1 + $\mu _y$)y, entonces el error cometido al realizar la operación suma es:
	\[ (1 + \mu _x)x + (1 + \mu _y)y = x + y + \mu _xx + \mu _yy = (x+y) \left( 1 + \frac{\mu _xx + \mu _yy}{x+y} \right) = (x+y) \left( 1 + \frac{x}{x+y} \mu _x + \frac{y}{x+y} \mu _y \right)  \]
	Luego
	\[ \mu _{x+y} = \frac{x}{x+y} \mu _x + \frac{y}{x+y} \mu _y \]
	Además, si x e y tienen el mismo signo se puede controlar el error relativo:
	\[ \vert \mu _{x+y} \vert \leq \vert \mu _x \vert + \vert \mu _y \vert \]
	Aunque si x e y tienen signos opuestos, $\mu _{x+y}$ puede dispararse si $x + y \approx 0$, generándose un error relativo enorme, conocido como $\textbf{error de cancelación}$. 
	\item Multiplicación.\\
	Los errores relativos de x e y son pequeños.
	\[ (1 + \mu _x)x \cdot (1 + \mu _y)y = (1 + \mu _x + \mu _y + \mu _x \mu _y)xy \approx (1 + \mu _x + \mu _y)xy \]
	Luego
	\[ \mu _{xy} \approx \mu _x + \mu _y \]
	\item División.\\
	Los errores relativos de x e y son pequeños, siendo $y \neq 0$.
	\[ \frac{(1 + \mu _x)x}{(1 + \mu _y)y} = \frac{x}{y}(1 + \mu _x)(1 - \mu _y + \mu _y^2 - \mu _y^3 + \cdots) \approx \frac{x}{y}(1 + \mu _x - \mu _y) \]
	Por lo que
	\[ \mu _{x/y} \approx \mu _x - \mu _y \]
	\end{nlist}	
	
	
\part{Tema 2. Resolución numérica de sistemas de ecuaciones lineales}
En este tema vamos a hacer un tratamiento numérico de los sistemas de ecuaciones lineales. Vamos a usar dos métodos:
	\begin{nlist}
	\item Métodos directos: Gauss, versiones y factorizaciones.
	\item Métodos iterativos: Jacobi y Gauss-Seidel.
	\end{nlist}

	\section{Métodos directos: Gauss y versiones, factorización de matrices}
	Si tenemos un sistema $N \times N$ unisolvente, con N grande, la regla de Cramer es ineficiente, ya que tenemos un gran número de operaciones elementales. La regla de Cramer hace N + 1 determinantes + N divisiones, lo cual lleva a
\[ (N+1)(N!N-1)+N = (N+1)!N-1 \; \; operaciones \]
		\subsection{Sistemas triangulares}
		Sean la matriz $U \in \mathbb{R}^{N \times N}$ $\textbf{triangular superior}$ con elementos diagonales no nulos, el vector de incógnitas $x \in \mathbb{R}^N$ y el vector de términos independientes b, tenemos el siguiente sistema triangular: $\textbf{Ux = b}$. Este sistema se resuelve por $\textbf{sustitución hacia atrás}$ y el algoritmo para resolverlo es:
		\[ x_i = \frac{1}{u_{ii}} \left( b_i - \sum _{j=i+1}^N u_{ij}x_j \right), \; \; con \; \; i = N,...,1 \]
		Sea la matriz $L \in \mathbb{R}^{N \times N}$  $\textbf{triangular inferior}$ con elementos diagonales no nulos, el vector de incógnitas $x \in \mathbb{R}^N$ y el vector de términos independientes b, tenemos el siguiente sistema triangular: $\textbf{Lx = b}$. Este sistema se resuelve por $\textbf{sustitución hacia adelante}$ y el algoritmo para resolverlo es:
		\[ x_i = \frac{1}{l_{ii}} \left( b_i - \sum _{j=1}^{i-1} l_{ij}x_j \right), \; \; con \; \; i = 1,...,N \]
		
		\subsection{Métodos de Gauss y Gauss-Jordan. Pivotaje}
		\begin{nlist}
		\item $\textbf{Método de Gauss}$\\
		Si tenemos un sistema Ax = b unisolvente, con el método de Gauss obtenemos otro sistema $\textbf{Ux = c}$ equivalente con U triangular superior, que ya hemos visto como se resuelve.\\
		Los datos son $N \geq 1, A \in \mathbb{R}^{N \times N}, b \in \mathbb{R}^N$.\\
		Sea $A^{(1)}$ := A. Suponemos que $a_{kk}^{(k)} \neq 0 \; con \; k = 1,...,N$ (en caso contrario hemos terminado y no es posible llegar a un sistema triangular equivalente). Definimos recursivamente los multiplicadores:
		\[ m_{ik} := \frac{a_{ik}^{(k)}}{a_{kk}^{(k)}}, \; \; con \; \; i = k+1,...,N \]
e
\[ a_{ij}^{(k+1)} := a_{ij}^{(k)} - m_{ik}a{kj}^{(k)}, \; \; con \; \; i = k+1,...,N; \; j = k,...,N \]
\[ b_i^{(k+1)} := b_i^{(k)} - m_{ik}a_{kj}^{(k)}, \; \; con \; \; i = k+1,...,N \]
Por lo que obtenemos un sistema triangular superior equivalente que se resuelve por sustitución hacia atrás:
\[ Ux = c \]
donde $U := A^{(N)}$, $c := b^{(N)}$.
		
		\item $\textbf{Método de Gauss-Jordan}$
		
		
		\end{nlist}
		
		\subsection{Métodos de factorización}
		
	\section{Métodos iterativos: métodos de Jacobi y Gauss-Seidel}
		\subsection{Métodos iterativos: convergencia y consistencia}
		\subsection{Generación de métodos iterativos. Jacobi y Gauss-Seidel}
	\section{Análisis de error}
	


